{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb6c9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pymongo-4.4.1-cp39-cp39-win_amd64.whl (408 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.3.0 pymongo-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ae5a7",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion Pipeline:\n",
    "- a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "- b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "- c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88337441",
   "metadata": {},
   "source": [
    "##### Data collection from various sources (Here mongodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc84b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "# Provide the mongodb localhost url to connect python to mongodb.\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentVariable:\n",
    "    mongo_db_url:str = os.getenv(\"MONGO_DB_URL\")     # Providing url and access key\n",
    "    aws_access_key_id:str = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_access_secret_key:str = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "env_var = EnvironmentVariable()\n",
    "mongo_client = pymongo.MongoClient(env_var.mongo_db_url)\n",
    "TARGET_COLUMN = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sensor.config import mongo_client\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "def get_collection_as_dataframe(database_name:str,collection_name:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description: This function return collection as dataframe\n",
    "    =========================================================\n",
    "    Params:\n",
    "    database_name: database name\n",
    "    collection_name: collection name\n",
    "    =========================================================\n",
    "    return Pandas dataframe of a collection\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(list(mongo_client[database_name][collection_name].find()))\n",
    "    if \"_id\" in df.columns:\n",
    "        df = df.drop(\"_id\",axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae250f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataIngestion:\n",
    "    '''\n",
    "    Make sure to to import above functions as modules so that every things \n",
    "    works, here config entity is basically the above code that is saved as\n",
    "    config, \n",
    "    '''\n",
    "    def __init__(self,data_ingestion_config:config_entity.DataIngestionConfig ):\n",
    "        self.data_ingestion_config = data_ingestion_config\n",
    "\n",
    "    def initiate_data_ingestion(self)->artifact_entity.DataIngestionArtifact:\n",
    "        \n",
    "        # Exporting collection data as pandas dataframe\n",
    "        df:pd.DataFrame  = utils.get_collection_as_dataframe(\n",
    "            database_name=self.data_ingestion_config.database_name, \n",
    "            collection_name=self.data_ingestion_config.collection_name)\n",
    "            \n",
    "        #replace na with Nan\n",
    "        df.replace(to_replace=\"na\",value=np.NAN,inplace=True)\n",
    "        #Save data in feature store\n",
    "        \n",
    "        #Create feature store folder if not available\n",
    "        feature_store_dir = os.path.dirname(self.data_ingestion_config.feature_store_file_path)\n",
    "        os.makedirs(feature_store_dir,exist_ok=True)\n",
    "        #Save df to feature store folder\n",
    "        df.to_csv(path_or_buf=self.data_ingestion_config.feature_store_file_path,index=False,header=True)\n",
    "        #split dataset into train and test set\n",
    "        train_df,test_df = train_test_split(df,test_size=self.data_ingestion_config.test_size,random_state=42)\n",
    "            \n",
    "        #create dataset directory folder if not available\n",
    "        dataset_dir = os.path.dirname(self.data_ingestion_config.train_file_path)\n",
    "        os.makedirs(dataset_dir,exist_ok=True)\n",
    "\n",
    "        #Save df to feature store folder\n",
    "        train_df.to_csv(path_or_buf=self.data_ingestion_config.train_file_path,index=False,header=True)\n",
    "        test_df.to_csv(path_or_buf=self.data_ingestion_config.test_file_path,index=False,header=True)\n",
    "            \n",
    "        #Prepare artifact\n",
    "\n",
    "        data_ingestion_artifact = artifact_entity.DataIngestionArtifact(\n",
    "            feature_store_file_path=self.data_ingestion_config.feature_store_file_path,\n",
    "            train_file_path=self.data_ingestion_config.train_file_path, \n",
    "            test_file_path=self.data_ingestion_config.test_file_path)\n",
    "\n",
    "        return data_ingestion_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9872027",
   "metadata": {},
   "source": [
    "## 2. Model Training:\n",
    "- a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and evaluate its performance.\n",
    "- b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction.\n",
    "- c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import artifact_entity,config_entity\n",
    "from typing import Optional\n",
    "import os,sys \n",
    "from xgboost import XGBClassifier\n",
    "from above import the code\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,model_trainer_config:config_entity.ModelTrainerConfig,\n",
    "                data_transformation_artifact:artifact_entity.DataTransformationArtifact\n",
    "                ):\n",
    "        \n",
    "        self.model_trainer_config=model_trainer_config\n",
    "        self.data_transformation_artifact=data_transformation_artifact\n",
    "\n",
    "    def train_model(self,x,y):\n",
    "        xgb_clf =  XGBClassifier()\n",
    "        xgb_clf.fit(x,y)\n",
    "        return xgb_clf\n",
    "\n",
    "    def initiate_model_trainer(self,)->artifact_entity.ModelTrainerArtifact:\n",
    "       \n",
    "        train_arr = utils.load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_train_path)\n",
    "        test_arr = utils.load_numpy_array_data(file_path=self.data_transformation_artifact.transformed_test_path)\n",
    "\n",
    "        x_train,y_train = train_arr[:,:-1],train_arr[:,-1]\n",
    "        x_test,y_test = test_arr[:,:-1],test_arr[:,-1]\n",
    "\n",
    "        model = self.train_model(x=x_train,y=y_train)\n",
    "\n",
    "        yhat_train = model.predict(x_train)\n",
    "        f1_train_score  =f1_score(y_true=y_train, y_pred=yhat_train)\n",
    "\n",
    "        yhat_test = model.predict(x_test)\n",
    "        f1_test_score  =f1_score(y_true=y_test, y_pred=yhat_test)\n",
    "            \n",
    "        #check for overfitting or underfiiting or expected score\n",
    "        if f1_test_score<self.model_trainer_config.expected_score:\n",
    "            raise Exception(f\"Model is not good as it is not able to give \\\n",
    "            expected accuracy: {self.model_trainer_config.expected_score}: model actual score: {f1_test_score}\")\n",
    "\n",
    "        diff = abs(f1_train_score-f1_test_score)\n",
    "\n",
    "        if diff>self.model_trainer_config.overfitting_threshold:\n",
    "            raise Exception(f\"Train and test score diff: {diff} is more than overfitting threshold {self.model_trainer_config.overfitting_threshold}\")\n",
    "\n",
    "        #save the trained model\n",
    "        utils.save_object(file_path=self.model_trainer_config.model_path, obj=model)\n",
    "\n",
    "        #prepare artifact\n",
    "        model_trainer_artifact  = artifact_entity.ModelTrainerArtifact(model_path=self.model_trainer_config.model_path, \n",
    "        f1_train_score=f1_train_score, f1_test_score=f1_test_score)\n",
    "        return model_trainer_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b0d40",
   "metadata": {},
   "source": [
    "##### There are lots of code missing so please refer to this git hub for complete [Link](https://github.com/geijinchan/In-Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42700fca",
   "metadata": {},
   "source": [
    "## 3. Model Validation:\n",
    "- a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n",
    "- b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem.\n",
    "- c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "        def initiate_model_evaluation(self)->artifact_entity.ModelEvaluationArtifact:\n",
    "            #if saved model folder has model the we will compare \n",
    "            #which model is best trained or the model from saved model folder\n",
    "\n",
    "        \n",
    "            latest_dir_path = self.model_resolver.get_latest_dir_path()\n",
    "            if latest_dir_path==None:\n",
    "                model_eval_artifact = artifact_entity.ModelEvaluationArtifact(is_model_accepted=True,\n",
    "                improved_accuracy=None)\n",
    "                return model_eval_artifact\n",
    "\n",
    "\n",
    "            #Finding location of transformer model and target encoder\n",
    "            transformer_path = self.model_resolver.get_latest_transformer_path()\n",
    "            model_path = self.model_resolver.get_latest_model_path()\n",
    "            target_encoder_path = self.model_resolver.get_latest_target_encoder_path()\n",
    "\n",
    "            #Previous trained  objects\n",
    "            transformer = load_object(file_path=transformer_path)\n",
    "            model = load_object(file_path=model_path)\n",
    "            target_encoder = load_object(file_path=target_encoder_path)\n",
    "            \n",
    "\n",
    "            #Currently trained model objects\n",
    "            current_transformer = load_object(file_path=self.data_transformation_artifact.transform_object_path)\n",
    "            current_model  = load_object(file_path=self.model_trainer_artifact.model_path)\n",
    "            current_target_encoder = load_object(file_path=self.data_transformation_artifact.target_encoder_path)\n",
    "            \n",
    "\n",
    "\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            target_df = test_df[TARGET_COLUMN]\n",
    "            y_true =target_encoder.transform(target_df)\n",
    "            # accuracy using previous trained model\n",
    "            \n",
    "            input_feature_name = list(transformer.feature_names_in_)\n",
    "            input_arr =transformer.transform(test_df[input_feature_name])\n",
    "            y_pred = model.predict(input_arr)\n",
    "            print(f\"Prediction using previous model: {target_encoder.inverse_transform(y_pred[:5])}\")\n",
    "            previous_model_score = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "           \n",
    "            # accuracy using current trained model\n",
    "            input_feature_name = list(current_transformer.feature_names_in_)\n",
    "            input_arr =current_transformer.transform(test_df[input_feature_name])\n",
    "            y_pred = current_model.predict(input_arr)\n",
    "            y_true =current_target_encoder.transform(target_df)\n",
    "            print(f\"Prediction using trained model: {current_target_encoder.inverse_transform(y_pred[:5])}\")\n",
    "            current_model_score = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "                \n",
    "            model_eval_artifact = artifact_entity.ModelEvaluationArtifact(is_model_accepted=True,\n",
    "            improved_accuracy=current_model_score-previous_model_score)\n",
    "            return model_eval_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529cbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8749fdf",
   "metadata": {},
   "source": [
    "## 4. Deployment Strategy:\n",
    "- a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.\n",
    "- b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.\n",
    "- c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aaa429",
   "metadata": {},
   "source": [
    "Deployment Strategy:\n",
    "- Creating a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions would involve designing an API or service to handle user requests and provide recommendations based on the model's predictions.\n",
    "- Developing a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure would typically involve using infrastructure-as-code tools like Terraform or AWS CloudFormation to provision the necessary resources, containerization technologies like Docker, and deployment automation tools like Jenkins or GitLab CI/CD.\n",
    "\n",
    "- Designing a monitoring and maintenance strategy for deployed models involves setting up monitoring tools to track model performance, resource utilization, and potential issues. It may also include implementing strategies for model retraining and updating based on new data or changes in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df702900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
